{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- use this command in cmd - spark-shell -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e1f5cd-97cf-4070-9f57-38498f9ec8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e0f178-50af-4903-b9dd-603859ee3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66574000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96071278-b314-411f-b494-98497dbe2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2ee3e5-5adc-4bfd-af6c-d5a1f1d8fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262e9f82-c558-4fb1-b493-a79bc033b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input file and Calculating words count\n",
    "text_file = sc.textFile(\"input.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335f9d1f-c299-46cb-bcde-5c4cd70cb551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt: 1\n",
      "others: 1\n",
      "happy,: 1\n",
      "but: 1\n",
      "really: 1\n",
      "I: 3\n",
      "happy: 2\n",
      "because: 1\n",
      "saw: 1\n",
      "were: 1\n",
      "wasnâ€™t: 1\n"
     ]
    }
   ],
   "source": [
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22cbe224-63c3-4fba-96fa-aacee42c33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8fadb-ac76-4522-8a6f-8030d8bd6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark: This line imports the findspark library, which is used to locate the Spark installation on your system.\n",
    "findspark.init(): This line initializes the Spark environment. It adds the Spark installation to the Python path so that Python can locate it.\n",
    "import pyspark: This line imports the PySpark library, which is the Python API for Spark. It allows you to interact with Spark using Python.\n",
    "from pyspark.sql import SparkSession: This line imports the SparkSession class from the pyspark.sql module. SparkSession is the entry point to Spark SQL functionality and allows the creation and management of DataFrame objects.\n",
    "spark=SparkSession.builder.getOrCreate(): This line creates a SparkSession object named spark. If a SparkSession already exists, it returns that session; otherwise, it creates a new one.\n",
    "sc=spark.sparkContext: This line creates a SparkContext object named sc. SparkContext is the entry point for low-level APIs, including RDD operations.\n",
    "text_file = sc.textFile(\"input.txt\"): This line reads the input file named \"input.txt\" and creates an RDD (Resilient Distributed Dataset) named text_file. An RDD is a distributed collection of elements that can be operated on in parallel.\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y): This line performs a series of transformations and actions on the RDD to calculate the word counts.\n",
    "flatMap(lambda line: line.split(\" \")): Splits each line into words and flattens them into a single list.\n",
    "map(lambda word: (word, 1)): Maps each word to a tuple (word, 1), where 1 represents the count of the word.\n",
    "reduceByKey(lambda x, y: x + y): Reduces by key, summing up the counts for each word.\n",
    "output = counts.collect(): This line triggers the execution of the RDD transformations and actions and collects the results into a local Python list named output.\n",
    "for (word, count) in output:: This line iterates over each tuple in the output list, where each tuple contains a word and its corresponding count.\n",
    "print(\"%s: %i\" % (word, count)): This line prints each word and its count in the format \"word: count\".\n",
    "sc.stop(): This line stops the SparkContext, releasing the resources associated with it.\n",
    "spark.stop(): This line stops the SparkSession, releasing the resources associated with it.\n",
    "Overall, this code reads an input text file, calculates the word counts using Spark's RDD API, and prints the word counts to the console."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
